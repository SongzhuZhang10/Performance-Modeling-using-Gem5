/*
 * Copyright (c) 2017 Jason Lowe-Power
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are
 * met: redistributions of source code must retain the above copyright
 * notice, this list of conditions and the following disclaimer;
 * redistributions in binary form must reproduce the above copyright
 * notice, this list of conditions and the following disclaimer in the
 * documentation and/or other materials provided with the distribution;
 * neither the name of the copyright holders nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */

/**
 * This file contains a simple example MSI protocol.
 *
 * The protocol in this file is based on the MSI protocol found in
 * A Primer on Memory Consistency and Cache Coherence
 *      Daniel J. Sorin, Mark D. Hill, and David A. Wood
 *      Synthesis Lectures on Computer Architecture 2011 6:3, 141-149
 *
 * Table 8.1 contains the transitions and actions found in this file and
 * section 8.2.4 explains the protocol in detail.
 *
 * See Learning gem5 Part 3: Ruby for more details.
 */

/// Declare a machine with type L1Cache.
// "MachineType:L1Cache" means that we name this state machine L1Cache.
// "MSI cache" in this declaration is a description of this state machine.
// There are many cases in SLICC where you must include a description to
// go along with the variable.
machine(MachineType:L1Cache, "MSI cache")
    : Sequencer *sequencer; // Incoming request from CPU to this machine
      CacheMemory *cacheMemory; // This stores the data and cache states
      bool send_evictions; // Needed to support O3 CPU and mwait

      // Other declarations
      // Message buffers are required to send and receive data from the Ruby
      // network. The from/to and request/response can be confusing!
      // Virtual networks are needed to prevent deadlock (e.g., it is bad if a
      // response gets stuck behind a stalled request). In this protocol, we are
      // using three virtual networks. The highest priority is responses,
      // followed by forwarded requests, then requests have the lowest priority.

      // Requests *to* the directory
      MessageBuffer * requestToDir, network="To", virtual_network="0",
            vnet_type="request";
      // Responses *to* the directory or other caches
      MessageBuffer * responseToDirOrSibling, network="To", virtual_network="2",
            vnet_type="response";

      // Requests *from* the directory to this cache controller for fwds, invs, and put acks.
      MessageBuffer * forwardFromDir, network="From", virtual_network="1",
            vnet_type="forward";
      // Responses *from* the directory and other caches for this cache's reqs.
      MessageBuffer * responseFromDirOrSibling, network="From",
            virtual_network="2", vnet_type="response";

      // This is all of the incoming requests from the core via the sequencer
      MessageBuffer * mandatoryQueue;
{
    // Declare the states that this cache will use. These are both stable
    // states (no underscore) and transient states (with underscore). Letters
    // after the underscores are superscript in Sorin et al.
    // Underscores and "desc" are used when generating HTML tables.
    // Access permissions are used for functional accesses. For reads, the
    // functional access reads *all* of the blocks with a matching address that
    // have read-only or read-write permission. For functional writes, all
    // blocks are updated with new data if they have busy, read-only, or
    // read-write permission.
    // These states come directly from the left column of Table 8.3.
    state_declaration(State, desc="Cache states") {
        // The access permission is used for functional accesses to the cache.
        // For example, reading files in SE mode will directly load them into memory.
        I,      AccessPermission:Invalid,
                    desc="Not present/Invalid";

        // States moving out of I
        IS_D,   AccessPermission:Invalid,
                    desc="Invalid, moving to S, waiting for data";
        IM_AD,  AccessPermission:Invalid,
                    desc="Invalid, moving to M, waiting for acks and data";
        // Note that when data has arrived but acks haven't, the access
        // permission is set to busy.
        IM_A,   AccessPermission:Busy,
                    desc="Invalid, moving to M, waiting for acks";

        S,      AccessPermission:Read_Only,
                    desc="Shared. Read-only, other caches may have the block";

        // States moving out of S
        SM_AD,  AccessPermission:Read_Only,
                    desc="Shared, moving to M, waiting for acks and 'data'";
        SM_A,   AccessPermission:Read_Only,
                    desc="Shared, moving to M, waiting for acks";

        M,      AccessPermission:Read_Write,
                    desc="Modified. Read & write permissions. Owner of block";

        // States moving to Invalid
        MI_A,   AccessPermission:Busy,
                    desc="Was modified, moving to I, waiting for put ack";
        SI_A,   AccessPermission:Busy,
                    desc="Was shared, moving to I, waiting for put ack";
        II_A,   AccessPermission:Invalid,
                    desc="Was invalid, moving to I, waiting for put ack";
    }

    /**
     * Declare all of the events that are triggered by incoming messages for this cache controller.
     * These events come directly from the first row in Table 8.3.
     */
    // Events that can be triggered on incoming messages.
    // These are the events that will trigger transitions.
    enumeration(Event, desc="Cache events") {
        // From the processor/sequencer/mandatory queue
        Load,           desc="Load from processor";
        Store,          desc="Store from processor";

        // Internal event (only triggered from processor requests)
        Replacement,    desc="Triggered when block is chosen as victim";

        // Forwarded reqeust from other cache via dir on the forward network
        
        FwdGetS,        desc="Directory sent us a request to satisfy GetS. ";
                             //We must have the block in M to respond to this.

        FwdGetM,        desc="Directory sent us a request to satisfy GetM. ";
                             //We must have the block in M to respond to this.
                             
        Inv,            desc="Invalidate from the directory.";
        PutAck,         desc="Response from directory after we issue a put. ";
                             //This must be on the fwd network to avoid deadlock.

        // Responses from directory
        DataDirNoAcks,  desc="Data from directory (acks = 0)";
        DataDirAcks,    desc="Data from directory (acks > 0)";

        // Responses from other caches
        DataOwner,      desc="Data from owner";
        InvAck,         desc="Invalidation ack from other cache after Inv";

        // Special internally triggered event to simplify implementation
        LastInvAck,     desc="Triggered after the last ack is received";
    }

    // A structure for the cache entry. This stores the cache data and state
    // as defined above. You can put any other information here you like.
    // The AbstractCacheEntry is defined in
    // src/mem/ruby/slic_interface/AbstractCacheEntry.hh
    // If you want to use any of the functions in the abstract entry,
    // declare them here.
    structure(Entry, desc="Cache entry", interface="AbstractCacheEntry") {
        State CacheState,        desc="cache state";
        DataBlock DataBlk,       desc="Data in the block";
    }

    // TBE is the "transaction buffer entry". This stores information needed
    // during transient states. This is *like* an MSHR. It functions as an MSHR
    // in this protocol, but the entry is also allocated for other uses.
    structure(TBE, desc="Entry for transient requests") {
        State TBEState,         desc="State of block";
        DataBlock DataBlk,      desc="Data for the block. Needed for MI_A";
        // The AcksOutstanding is used for the transitions where other controllers
        // send acks instead of the data.
        int AcksOutstanding, default=0, desc="Number of acks left to receive.";
    }

    // Table of TBEs. This is defined externally in
    // src/mem/ruby/structures/TBETable.hh. It is templatized on the TBE
    // structure defined above.
    // The external="yes" tells SLICC to not look for the definition of this structure.
    structure(TBETable, external="yes") {
        // Declare the member functions of the class TBETable.
        // These functions will be used later.
        TBE lookup(Addr);
        void allocate(Addr);
        void deallocate(Addr);
        bool isPresent(Addr);
    }

    /*************************************************************************/
    // Some declarations of member functions and member variables.

    // NOTE: SLICC mangles names with the machine type. Thus, the TBE declared
    //       above will be L1Cache_TBE in C++.
    // We also have to pass through a parameter to the machine to the TBETable.
    TBETable TBEs, template="<L1Cache_TBE>", constructor="m_number_of_TBEs";

    // Declare all of the functions of the AbstractController that we may use
    // in this file.

    // Functions from clocked object
    Tick clockEdge();

    // Functions we must use to set things up for the transitions to execute correctly.
    // These set/unset functions are used to populate the implicit variables used
    // in actions. This is required when a transition has multiple actions.
    void set_cache_entry(AbstractCacheEntry a);
    void unset_cache_entry();
    void set_tbe(TBE b);
    void unset_tbe();

    // If you have multiple memory channels, this function tells
    // you the mapping between addresses and memory controllers.
    // In a real implementation, the mapping might be fixed
    // at design time, but this function gives us flexibility at runtime.
    /** 
     * This function  allows us to change the address mappings for banked directories
     * or caches at runtime so we don’t have to hardcode them in the SLICC file.
     */
    MachineID mapAddressToMachine(Addr addr, MachineType mtype);

    // This function looks up the cache entry.
    Entry getCacheEntry(Addr address), return_by_pointer="yes" {
        // Needs a pointer so that the cache entry can be updated in actions
        return static_cast(Entry, "pointer", cacheMemory.lookup(address));
    }

    /*************************************************************************/
    // Functions that we need to define/override to use our specific structures
    // in this implementation.

    // Required function for getting the current state of the block.
    // This is called on the block to decide which transition to execute when an event is triggered.
    // Usually, you return the state in the TBE or cache entry, whichever is valid.
    State getState(TBE tbe, Entry cache_entry, Addr addr) {
        // The TBE state will override the state in cache memory, if it's valid
        if (is_valid(tbe)) { return tbe.TBEState; }
        // If the cache entry is valid, it holds the state
        else if (is_valid(cache_entry)) { return cache_entry.CacheState; }
        // If the block isn't present, then its state must be I.
        else { return State:I; }
    }


    // Required function for setting the current state of the block.
    // This is called from the transition to set the ending state.
    // Needs to set both the TBE and the cache entry state.
    // This is also called when transitioning to I so it's possible the TBE and/
    // or the cache_entry is invalid.
    // This is called at the end of the transition to set the final state on the block.
    void setState(TBE tbe, Entry cache_entry, Addr addr, State state) {
      if (is_valid(tbe)) { tbe.TBEState := state; }
      if (is_valid(cache_entry)) { cache_entry.CacheState := state; }
    }

    // Required function to override. Used for functional access to know where
    // the valid data is. NOTE: L1Cache_State_to_permission is automatically
    // created based on the access permissions in the state_declaration.
    // This is mangled by both the MachineType and the name of the state
    // declaration ("State" in this case).
    // This is used during functional access to decide whether or not to functionally access the block.
    AccessPermission getAccessPermission(Addr addr) {
        TBE tbe := TBEs[addr];
        if(is_valid(tbe)) {
            return L1Cache_State_to_permission(tbe.TBEState);
        }

        Entry cache_entry := getCacheEntry(addr);
        if(is_valid(cache_entry)) {
            return L1Cache_State_to_permission(cache_entry.CacheState);
        }

        return AccessPermission:NotPresent;
    }

    // Required function to override. Like above function, but sets the state.
    void setAccessPermission(Entry cache_entry, Addr addr, State state) {
        if (is_valid(cache_entry)) {
            cache_entry.changePermission(L1Cache_State_to_permission(state));
        }
    }

    // Required function to override for functionally reading/writing data.
    // It is possible the TBE has more up-to-date information, so check that first.
    // NOTE: testAndRead/Write defined in src/mem/ruby/slicc_interface/Util.hh
    void functionalRead(Addr addr, Packet *pkt) {
        TBE tbe := TBEs[addr];
        if(is_valid(tbe)) {
            testAndRead(addr, tbe.DataBlk, pkt);
        } else {
            testAndRead(addr, getCacheEntry(addr).DataBlk, pkt);
        }
    }

    // Functionally write the data. Similarly, you may need to update the data
    // in both the TBE and the cache entry.
    int functionalWrite(Addr addr, Packet *pkt) {
        TBE tbe := TBEs[addr];
        if(is_valid(tbe)) {
            if (testAndWrite(addr, tbe.DataBlk, pkt)) {
                return 1;
            } else {
                return 0;
            }
        } else {
            if (testAndWrite(addr, getCacheEntry(addr).DataBlk, pkt)) {
                return 1;
            } else {
                return 0;
            }
        }
    }

    /*************************************************************************/
    // Input/output network definitions

    // Output ports This defines the message types that will flow across the
    // output buffers as defined above. These must be "to" networks.
    // "request_out" is the name we'll use later to send requests.
    // "RequestMsg" is the message type we will send (see MSI-msg.sm)
    // "requestToDir" is the name of the MessageBuffer out of which we are sending these requests.
    /**
     * This code essentially just renames requestToDir and responseToDirOrSibling
     * to request_out and response_out.
     * When we want to enqueue messages to these message buffers, we will use the
     * new names request_out and response_out.
     */
    out_port(request_out, RequestMsg, requestToDir);
    out_port(response_out, ResponseMsg, responseToDirOrSibling);

    // Input ports. The order here is/(can be) important. The code in each
    // in_port is executed in the order specified in this file (or by the rank
    // parameter). Thus, we must sort these based on the network priority.
    // In this cache, the order is
    //  1. responses from other caches (highest priority),
    //  2. forwarded requests,
    //  3. requests from the CPU (lowest priority).

    /**
     * All of the in_port code blocks are executed in order (or based on the priority if it is specified).
     * On each active cycle for the controller, the first in_port code is executed.
     * If it is successful, it is re-executed to see if there are other messages that can be consumed on the port.
     * If there are no messages or no events are triggered, then the next in_port code block is executed.
     */
    // Like the out_port above
    // "response_in" is the name we'll use later when we refer to this port
    // "ResponseMsg" is the type of message we expect on this port
    // "responseFromDirOrSibling" is the name of the buffer this in_port is
    // connected to for responses from other caches and the directory.

    // This in_port block is for the response network which has the
    // highest priority to transfer responses from directory or other caches
    // to this L1 cache controller.
    in_port(response_in, ResponseMsg, responseFromDirOrSibling) {
        // NOTE: You have to check to make sure the message buffer has a valid
        // message at the head. The code in in_port is executed either way.
        /**
         * Check the message buffer to see if there are any messages to be processed.
         * If not, then this in_port code block is skipped and the next one is executed.
         */
        if (response_in.isReady(clockEdge())) {
            // Peek is a special function. Any code inside a peek statement has
            // a special variable declared and populated: in_msg. This contains
            // the message (of type ResponseMsg in this case) at the head of the port.
            // "response_in" is the port we want to peek into.
            // "ResponseMsg" is the type of message we expect.
            peek(response_in, ResponseMsg) {
                // Grab the entry and tbe if they exist.
                Entry cache_entry := getCacheEntry(in_msg.addr);
                TBE tbe := TBEs[in_msg.addr];
                // The TBE must exist since this is a response and we need to
                // be able to check the remaining acks.
                assert(is_valid(tbe));

                // If the response is from the directory...
                if (machineIDToMachineType(in_msg.Sender) == MachineType:Directory) {
                    if (in_msg.Type != CoherenceResponseType:Data) {
                        error("Directory should only reply with data");
                    }
                    // Take the in_msg acks and add (sub) the Acks we've seen.
                    // The InvAck will decrement the acks we're waiting for in
                    // tbe.AcksOutstanding to below 0 when we haven't gotten the
                    // dir resp yet. So, if this is 0, we don't need to wait
                    assert(in_msg.Acks + tbe.AcksOutstanding >= 0);
                    if (in_msg.Acks + tbe.AcksOutstanding == 0) {
                        trigger(Event:DataDirNoAcks, in_msg.addr, cache_entry,
                                tbe);
                    } else {
                        // If it's not 0, then we need to wait for more acks
                        // and we'll trigger LastInvAck later.
                        trigger(Event:DataDirAcks, in_msg.addr, cache_entry,
                                tbe);
                    }
                } else {
                    // The response is from another cache.
                    if (in_msg.Type == CoherenceResponseType:Data) {
                        trigger(Event:DataOwner, in_msg.addr, cache_entry,
                                tbe);
                    } else if (in_msg.Type == CoherenceResponseType:InvAck) {
                        DPRINTF(RubySlicc, "Got inv ack. %d left\n",
                                tbe.AcksOutstanding);
                        if (tbe.AcksOutstanding == 1) {
                            // If there is exactly one ack remaining then we
                            // know it is the last ack.
                            trigger(Event:LastInvAck, in_msg.addr, cache_entry,
                                    tbe);
                        } else {
                            trigger(Event:InvAck, in_msg.addr, cache_entry,
                                    tbe);
                        }
                    } else {
                        error("Unexpected response from other cache");
                    }
                }
            }
        }
    }

    // Forward requests for other caches.
    // in_port logic for the forward network
    in_port(forward_in, RequestMsg, forwardFromDir) {
        if (forward_in.isReady(clockEdge())) {
            peek(forward_in, RequestMsg) {
                // Grab the entry and tbe if they exist.
                Entry cache_entry := getCacheEntry(in_msg.addr);
                TBE tbe := TBEs[in_msg.addr];

                if (in_msg.Type == CoherenceRequestType:GetS) {
                    // This is a special function that will trigger a
                    // transition (as defined below). It *must* have these
                    // parameters.
                    trigger(Event:FwdGetS, in_msg.addr, cache_entry, tbe);
                } else if (in_msg.Type == CoherenceRequestType:GetM) {
                    trigger(Event:FwdGetM, in_msg.addr, cache_entry, tbe);
                } else if (in_msg.Type == CoherenceRequestType:Inv) {
                    trigger(Event:Inv, in_msg.addr, cache_entry, tbe);
                } else if (in_msg.Type == CoherenceRequestType:PutAck) {
                    trigger(Event:PutAck, in_msg.addr, cache_entry, tbe);
                } else {
                    error("Unexpected forward message!");
                }
            }
        }
    }

    // The "mandatory queue" is the port/queue from the CPU or other processor.
    // This is *always* a RubyRequest
    // The "mandatory queue" is the lowest priority queue. So, it must be lowest
    // in the state machine file.
    // Message type RubyRequest is specified in src/mem/protocol/RubySlicc_Types.sm
    // RubyRequest contains two different addresses:
    //  1. the LineAddress (cache-block aligned) and
    //  2. the PhysicalAddress (which holds the original request’s address and may not
    // be cache-block aligned).
    // We only need the LineAddress.
    in_port(mandatory_in, RubyRequest, mandatoryQueue) {
        if (mandatory_in.isReady(clockEdge())) {
            // Block all requests if there is already an outstanding request
            // that has the same line address. This is unblocked when we
            // finally respond to the request (i.e., when the current request is complete).
            peek(mandatory_in, RubyRequest, block_on="LineAddress") {
                // NOTE: Using LineAddress here to promote smaller requests to
                // full cache block requests.
                Entry cache_entry := getCacheEntry(in_msg.LineAddress);
                TBE tbe := TBEs[in_msg.LineAddress];
                // Note that if there isn't a matching entry (i.e., for the given set/index number, there is no
                // tag values in the set matching the tag value of the line address.)
                // and no room in the cache,
                // then we need to find a victim.
                // If
                //  a. the cache entry for this line is invalid, and
                //  b. there are no more entries available in the set,
                //      then we need to evict another entry.
                // The is_invalid checks if there is any one of the cache blocks
                // in the set is invalid.
                if (is_invalid(cache_entry) &&
                        cacheMemory.cacheAvail(in_msg.LineAddress) == false ) {
                    // Find a victim make room for the block.
                    // The "cacheProbe" function looks at the cache set (multiple blocks per set) located in
                    // the given line address and queries the replacement policy to find a victim.
                    // It returns the PHYSICAL line address of the cache block entry to be replaced.
                    /**
                     * To get the victim address, we can use the cacheProbe function on the CacheMemory object.
                     * This function uses the parameterized replacement policy and returns the physical (line)
                     * address of the victim entry.
                     * in_msg.LineAddress: line address of the victim cache block
                     */
                    Addr addr := cacheMemory.cacheProbe(in_msg.LineAddress);
                    Entry victim_entry := getCacheEntry(addr);
                    TBE victim_tbe := TBEs[addr];
                    /**
                     * When we trigger the Replacement event, we use
                     *  1. the physical line address of the victim cache block,
                     *  2. the victim cache entry, and
                     *  3. the tbe.
                     * Thus, when we take actions in the replacement transitions, 
                     * we will be acting on the victim block, not the requesting block.
                     */
                    trigger(Event:Replacement, addr, victim_entry, victim_tbe);
                    // addr: physical line address of the victim cache block
                    // victim_entry: victim cache entry
                } else {
                    // If the request from the CPU is a load request or an instr fetch...
                    if (in_msg.Type == RubyRequestType:LD ||
                            in_msg.Type == RubyRequestType:IFETCH) {
                        trigger(Event:Load, in_msg.LineAddress, cache_entry,
                                tbe);
                    // If the request from the CPU is a store request...
                    } else if (in_msg.Type == RubyRequestType:ST) {
                        trigger(Event:Store, in_msg.LineAddress, cache_entry,
                                tbe);
                    } else {
                        error("Unexpected type from processor");
                    }
                }
            }
        }
    }


    /*************************************************************************/
    // Below are all of the actions that might be taken on a transition.

    // Each actions has a name, a shorthand, and a description.
    // The shorthand is used when generating the HTML tables for the protocol.
    // "\" in the shorthand that causes letters to be bold.
    // "_" inserts a space.
    // "^" makes the rest of the letters superscript.
    // The description is also shown in the HTML table when clicked

    // The first set of actions are things we will do to interact with the
    // rest of the system. Things like sending requests/responses.

    // Action blocks define a number of useful implicit variables.
    // These variables come straight from the trigger() call in the in_port blocks.
    // address: The address passed in the trigger (usually the in_msg.addr,
    //          though it can be different. E.g., on a replacement, it is the victim address).
    // cache_entry: The cache entry passed in the trigger call
    // tbe: The TBE passed in the trigger call
    action(sendGetS, 'gS', desc="Send GetS to the directory") {
        // Instead of populating in_msg, enqueue has an out_msg reference.
        // Whatever you set on out_msg is sent through the out port specified.
        // "request_out" is the port out of which we're sending the message.
        // "RequestMsg" is the type of message we're sending.
        // "1" is the latency (in cycles) the port waits before sending the message.
        enqueue(request_out, RequestMsg, 1) {
            // request_out: the message buffer to send the message
            // RequestMsg: the type of the message
            // 1: 1 cycle cache latency (latency of accessing the cache for a miss)
            // For the address of the request, we can use the |address| variable which
            // is automatically populated by the trigger function.
            out_msg.addr := address;
            // This type is defined in MSI-msg.sm for this protocol.
            out_msg.Type := CoherenceRequestType:GetS; // sending a GetS message
            // The destination may change depending on the address
            // across different directories; so, we need to query the network.
            // Specify the destination of the message, taking the address
            // and the machine type we are sending to.
            // This will look up the correct MachineID based on the address.
            // We call Destination.add because Destination is a NetDest object, or a bitmap of all MachineID.
            out_msg.Destination.add(mapAddressToMachine(address,
                                    MachineType:Directory));
            // See mem/ruby/protocol/RubySlicc_Exports.sm for possible sizes.
            out_msg.MessageSize := MessageSizeType:Control;
            // Set the reqeustor to be this machine so that we can get the response.
            out_msg.Requestor := machineID;
        }
    }

    action(sendGetM, "gM", desc="Send GetM to the directory") {
        enqueue(request_out, RequestMsg, 1) {
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:GetM;
            out_msg.Destination.add(mapAddressToMachine(address,
                                    MachineType:Directory));
            out_msg.MessageSize := MessageSizeType:Control;
            out_msg.Requestor := machineID;
        }
    }

    // NOTE: Clean evict. Required to keep the directory state up-to-date
    action(sendPutS, "pS", desc="Send PutS to the directory") {
        enqueue(request_out, RequestMsg, 1) {
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:PutS;
            out_msg.Destination.add(mapAddressToMachine(address,
                                    MachineType:Directory));
            out_msg.MessageSize := MessageSizeType:Control;
            out_msg.Requestor := machineID;
        }
    }

    action(sendPutM, "pM", desc="Send putM+data to the directory") {
        enqueue(request_out, RequestMsg, 1) {
            out_msg.addr := address;
            out_msg.Type := CoherenceRequestType:PutM;
            out_msg.Destination.add(mapAddressToMachine(address,
                                    MachineType:Directory));
            out_msg.DataBlk := cache_entry.DataBlk;
            out_msg.MessageSize := MessageSizeType:Data;
            out_msg.Requestor := machineID;
        }
    }

    action(sendCacheDataToReq, "cdR", desc="Send cache data to requestor") {
        // We have to peek into the request to see to whom we're sending the cache data.
        // If we are in both the peek and the enqueue block,
        // then we have access to both in_msg and out_msg.
        assert(is_valid(cache_entry));
        peek(forward_in, RequestMsg) {
            enqueue(response_out, ResponseMsg, 1) {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:Data;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.MessageSize := MessageSizeType:Data;
                // Set the sender to be this machine so that we can get the response.
                out_msg.Sender := machineID;
            }
        }
    }

    action(sendCacheDataToDir, "cdD", desc="Send the cache data to the dir") {
        enqueue(response_out, ResponseMsg, 1) {
            out_msg.addr := address;
            out_msg.Type := CoherenceResponseType:Data;
            out_msg.Destination.add(mapAddressToMachine(address,
                                    MachineType:Directory));
            out_msg.DataBlk := cache_entry.DataBlk;
            out_msg.MessageSize := MessageSizeType:Data;
            out_msg.Sender := machineID;
        }
    }

    action(sendInvAcktoReq, "iaR", desc="Send inv-ack to requestor") {
        peek(forward_in, RequestMsg) {
            enqueue(response_out, ResponseMsg, 1) {
                out_msg.addr := address;
                out_msg.Type := CoherenceResponseType:InvAck;
                out_msg.Destination.add(in_msg.Requestor);
                out_msg.DataBlk := cache_entry.DataBlk;
                out_msg.MessageSize := MessageSizeType:Control;
                out_msg.Sender := machineID;
            }
        }
    }

    action(decrAcks, "da", desc="Decrement the number of acks") {
        assert(is_valid(tbe));
        // Decrement the number of acks we are waiting for.
        // This is used when we get an invalidation ack from another cache
        // to track the total number of acks.
        tbe.AcksOutstanding := tbe.AcksOutstanding - 1;
        // This annotates the protocol trace
        // APPEND_TRANSITION_COMMENT makes debugging easier.
        // It takes a string, or something that can easily be converted
        // to a string (e.g., int) as a parameter.
        // It modifies the protocol trace output.
        // On each protocol trace line that executes this action, it will
        // print the total number of acks this cache is still waiting on.
        // This is useful since the number of remaining acks is part of the cache block state.
        // TODO: Figure out how to view the protocol trace.
        APPEND_TRANSITION_COMMENT("Acks: ");
        APPEND_TRANSITION_COMMENT(tbe.AcksOutstanding);
    }

    // An action to store the acks when we receive a message from the directory with an ack count.
    // For this action, we peek into the directory’s response message to get the number of acks
    // and store them in the TBE.
    action(storeAcks, "sa", desc="Store the needed acks to the TBE") {
        assert(is_valid(tbe));
        peek(response_in, ResponseMsg) {
            tbe.AcksOutstanding := in_msg.Acks + tbe.AcksOutstanding;
        }
        assert(tbe.AcksOutstanding > 0);
    }

    // Responses to CPU requests (e.g., hits and store acks)
    /**
     * -> Set of actions that respond to CPU requests on hits and misses. <-
     * Notify the sequencer (the interface between Ruby and the rest of gem5) of the new data.
     * In the case of a store, we give the sequencer a pointer to the data block so that
     * the sequencer updates the data in-place.
     * In each of these actions, it is vital that we call setMRU on the cache entry.
     * The setMRU function is what allows the replacement policy to know which blocks are most recently accessed.
     * If you leave out the setMRU call, the replacement policy will not operate correctly!
     * Calling readCallback function on the sequencer notifies the sequencer of the new data.
     * Calling writeCallback function on the sequencer allows it to write the data into the data block.
     * The read/writeCallback function take four parameters (the last parameter is optional):
     * 1. address,
     * 2. data block,
     * 3. boolean value for if the original request was a miss (true -> miss, false -> hit),
     * 4. an optional MachineType. It is used for tracking statistics on where the data for the request was found.
     *    It allows you to determine if the data comes from cache-to-cache transfers or from memory.
     */

    action(loadHit, "Lh", desc="Load hit") {
        assert(is_valid(cache_entry));
        // Set this entry as the most recently used for the replacement policy
        cacheMemory.setMRU(cache_entry);
        // Send the data back to the sequencer/CPU.
        // False indicates that a hit in this local cache has occured.
        // False also indicates that an "external hit" has not occured.
        sequencer.readCallback(address, cache_entry.DataBlk, false);
    }

    action(externalLoadHit, "xLh", desc="External load hit (was a miss)") {
        assert(is_valid(cache_entry));
        peek(response_in, ResponseMsg) {
            cacheMemory.setMRU(cache_entry);
            // Forward the type of machine that responded to this request
            // E.g., another cache or the directory. This is used for tracking
            // statistics.
            sequencer.readCallback(address, cache_entry.DataBlk, true,
                                   machineIDToMachineType(in_msg.Sender));
        }
    }

    action(storeHit, "Sh", desc="Store hit") {
        assert(is_valid(cache_entry));
        cacheMemory.setMRU(cache_entry);
        // The same as the read callback above.
        sequencer.writeCallback(address, cache_entry.DataBlk, false);
    }

    action(externalStoreHit, "xSh", desc="External store hit (was a miss)") {
        assert(is_valid(cache_entry));
        // TODO: Why use peek function here?
        peek(response_in, ResponseMsg) {
            cacheMemory.setMRU(cache_entry);
            sequencer.writeCallback(address, cache_entry.DataBlk, true,
                                   // Note: this could be the last ack.
                                   machineIDToMachineType(in_msg.Sender));
        }
    }

    /**
     * An action to forward evictions to the CPU.
     * This is required for gem5's out-of-order models to squash speculative loads when
     * the cache block is evicted before the load is committed.
     * Use the parameter specified at the top of the state machine file to check if this
     * action is needed or not.
     */
    action(forwardEviction, "e", desc="sends eviction notification to CPU") {
        if (send_evictions) {
            sequencer.evictionCallback(address);
        }
    }

    // Cache management actions that allocate and free cache entries and TBEs.
    /**
     * To create a new cache entry, we must have space in the CacheMemory object.
     * When allocating a new cache block, we call set_cache_entry so that,
     * in all actions proceeding allocateCacheBlock, the cache_entry variable will be valid.
     */

    action(allocateCacheBlock, "a", desc="Allocate a cache block") {
        assert(is_invalid(cache_entry));
        assert(cacheMemory.cacheAvail(address));
        // Create a new entry and update cache_entry to the new entry
        set_cache_entry(cacheMemory.allocate(address, new Entry));
    }

    action(deallocateCacheBlock, "d", desc="Deallocate a cache block") {
        assert(is_valid(cache_entry));
        cacheMemory.deallocate(address);
        // clear the cache_entry variable (now it's invalid)
        unset_cache_entry();
    }

    action(writeDataToCache, "wd", desc="Write data to the cache") {
        peek(response_in, ResponseMsg) {
            assert(is_valid(cache_entry));
            cache_entry.DataBlk := in_msg.DataBlk;
        }
    }

    action(allocateTBE, "aT", desc="Allocate TBE") {
        assert(is_invalid(tbe));
        TBEs.allocate(address);
        // this updates the tbe variable for other actions
        set_tbe(TBEs[address]);
    }

    action(deallocateTBE, "dT", desc="Deallocate TBE") {
        assert(is_valid(tbe));
        TBEs.deallocate(address);
        // this makes the tbe varible invalid
        unset_tbe();
    }
    // TODO: action copyDataFromCacheToTBE is missing here but is used on the tutorial website. Why?
    /**
     * An action that copies the data from the cache data block to the TBE.
     * This allows us to keep the data around even after removing the cache block until
     * we are sure that this cache no longer are responsible for the data.
     */
    // Queue management actions for managing the message buffers.

    // Ater the message has been satisfied, we need to pop the head message out of the buffer.
    action(popMandatoryQueue, "pQ", desc="Pop the mandatory queue") {
        // The dequeue function takes a single parameter that represents
        // the latency for the dequeue to take place.
        // Delaying the dequeue for a cycle prevents the in_port logic from
        // consuming another message from the same message buffer in a single cycle.
        mandatory_in.dequeue(clockEdge());
    }

    action(popResponseQueue, "pR", desc="Pop the response queue") {
        response_in.dequeue(clockEdge());
    }

    action(popForwardQueue, "pF", desc="Pop the forward queue") {
        forward_in.dequeue(clockEdge());
    }

    // Stalling actions
    /**
     * By leaving the action blank, it generates a “protocol stall” in the in_port
     * logic which stalls all messages from being processed in the current message
     * buffer and all lower priority message buffer.
     * Protocols using “z_stall” are usually simpler, but lower performance.
     * This is because a stall on a high priority buffer can stall many requests
     * that may not need to be stalled.
     */
    action(stall, "z", desc="Stall the incoming request") {
        // Do nothing. However, the transition must have some action to be
        // valid which is why this is needed.
        // NOTE: There are other more complicated but higher performing stalls
        // in Ruby like recycle() or stall_and_wait.
        // z_stall stalls everything in the queue behind this request.
        // TODO: Figure out how recycle() and stall_and_wait work
        // and if we can make use of them
    }


    /*************************************************************************/
    // These are the transition definition. These are simply each cell in the
    // table from Sorin et al. These are mostly in upper-left to bottom-right order.

    // Each transtion has (up to) 3 parameters,
    //  1. the current state,
    //  2. the triggering event,
    //  3. the final state.
    // Thus, the below transition reads "Move from state I on a Load event to state IS_D".
    // IS_D: Was invalid, going to shared, waiting for data.
    // Within the transition block, there is a set of actions to be taken during the transition.
    // These actions are executed atomically (i.e., all or nothing).
    // A simple transition in the MSI protocol is transitioning out of Invalid on a Load.
    transition(I, Load, IS_D) {
        // Make sure there is room in the cache to put the block whenever the miss returns.
        // Otherwise, we could deadlock.
        // The newly allocated entry is set to the entry that will be used in the rest of the actions.
        allocateCacheBlock;
        // We may need to track acks for this block and only the TBE holds an ack count.
        // Thus, we need to allocate both a TBE and a cache block.
        allocateTBE; // Used when we need to wait for acks from other caches.
        // Actually send the GetS request to the directory.
        sendGetS;
        // Since we have handled this request on the mandatory queue, we can pop
        // the head entry out of the mandatory queue.
        popMandatoryQueue;
    }

    transition(I, Store, IM_AD) {
        allocateCacheBlock;
        allocateTBE;
        sendGetM;
        popMandatoryQueue;
    }

    // You can use {} to specify multiple states or events for which the
    // transition applies. For instance, if we are in IS_D, then on any
    // of the following Events (Load, Store, Replacement, Inv), we should stall.
    // When there is no third parameter to transition, it means that we want
    // to stay in the beginning state (i.e., the state is not updated).
    /**
     * If the cache block is in state IS_D and there is a load, store, replacement, or invalidate,
     * then stall the protocol and do not transition out of the state.
     */
    transition(IS_D, {Load, Store, Replacement, Inv}) {
        stall;
    }

    // Similarly, on either DataDirNoAcks or DataOwner, we should go to S
    transition(IS_D, {DataDirNoAcks, DataOwner}, S) {
        writeDataToCache;
        deallocateTBE;
        // When a load miss occurs, externalLoadHit action should be taken.
        externalLoadHit;
        popResponseQueue;
    }

    transition({IM_AD, IM_A}, {Load, Store, Replacement, FwdGetS, FwdGetM}) {
        stall;
    }

    transition({IM_AD, SM_AD}, {DataDirNoAcks, DataOwner}, M) {
        // TODO: SM_AD cannot go to M with the event DataOwner. This may be a bug.
        writeDataToCache;
        deallocateTBE;
        externalStoreHit; // TODO: What does this do?
        popResponseQueue;
    }

    transition(IM_AD, DataDirAcks, IM_A) {
        writeDataToCache;
        storeAcks;
        popResponseQueue;
    }

    transition({IM_AD, IM_A, SM_AD, SM_A}, InvAck) {
        decrAcks;
        popResponseQueue;
    }

    transition({IM_A, SM_A}, LastInvAck, M) {
        // When transitioning to a stable state, deallocateTBE action should be taken.
        deallocateTBE;
        externalStoreHit;
        popResponseQueue;
    }

    transition({S, SM_AD, SM_A, M}, Load) {
        loadHit;
        popMandatoryQueue;
    }

    transition(S, Store, SM_AD) {
        // When transitioning to a transient state, allocateTBE action should be taken.
        allocateTBE;
        sendGetM;
        popMandatoryQueue;
    }

    transition(S, Replacement, SI_A) {
        sendPutS;
    }

    transition(S, Inv, I) {
        sendInvAcktoReq;
        forwardEviction;
        deallocateCacheBlock;
        popForwardQueue;
    }

    transition({SM_AD, SM_A}, {Store, Replacement, FwdGetS, FwdGetM}) {
        stall;
    }

    transition(SM_AD, Inv, IM_AD) {
        sendInvAcktoReq;
        popForwardQueue;
    }

    // DataDirAcks = Data from directory (acks > 0)
    transition(SM_AD, DataDirAcks, SM_A) {
        writeDataToCache;
        storeAcks;
        popResponseQueue;
    }

    // Both S->I and M->M can cause eviction in this implementation.
    transition(M, Store) {
        storeHit;
        forwardEviction;
        popMandatoryQueue;
    }

    transition(M, Replacement, MI_A) {
        // Send (PutM + data) to Dir
        sendPutM;
    }

    transition(M, FwdGetS, S) {
        // Send cache data to the requestor
        sendCacheDataToReq;
        // Send cache data to the directory
        sendCacheDataToDir;
        popForwardQueue;
    }

    transition(M, FwdGetM, I) {
        // Send cache data to the requestor
        sendCacheDataToReq;
        deallocateCacheBlock;
        popForwardQueue;
    }

    transition({MI_A, SI_A, II_A}, {Load, Store, Replacement}) {
        stall;
    }

    transition(MI_A, FwdGetS, SI_A) {
        sendCacheDataToReq;
        sendCacheDataToDir;
        popForwardQueue;
    }

    transition(MI_A, FwdGetM, II_A) {
        sendCacheDataToReq;
        popForwardQueue;
    }

    transition({MI_A, SI_A, II_A}, PutAck, I) {
        deallocateCacheBlock;
        popForwardQueue;
    }

    transition(SI_A, Inv, II_A) {
        sendInvAcktoReq;
        popForwardQueue;
    }

}
